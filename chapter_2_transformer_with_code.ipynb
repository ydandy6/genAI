{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydandy6/genAI/blob/main/chapter_2_transformer_with_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mr7FmYqAi6y2"
      },
      "source": [
        "## 예제 2.1 토큰화 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3K1on7SMnXj",
        "outputId": "a4b9cbbd-279d-4f1b-a0f8-fc5e32bb70db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_text_list:  ['나는', '최근', '파리', '여행을', '다녀왔다.', '파리가', '좋았어']\n",
            "str2idx:  {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다.': 4, '파리가': 5, '좋았어': 6}\n",
            "idx2str:  {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다.', 5: '파리가', 6: '좋았어'}\n",
            "input_ids:  [0, 1, 2, 3, 4, 5, 6]\n"
          ]
        }
      ],
      "source": [
        "# 띄어쓰기 단위로 분리\n",
        "input_text = \"나는 최근 파리 여행을 다녀왔다. 파리가 좋았어\"\n",
        "input_text_list = input_text.split()\n",
        "print(\"input_text_list: \", input_text_list)\n",
        "\n",
        "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
        "str2idx = {word:idx for idx, word in enumerate(input_text_list)}\n",
        "idx2str = {idx:word for idx, word in enumerate(input_text_list)}\n",
        "print(\"str2idx: \", str2idx)\n",
        "print(\"idx2str: \", idx2str)\n",
        "\n",
        "# 토큰을 토큰 아이디로 변환\n",
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(\"input_ids: \", input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CX95psBGjELL"
      },
      "source": [
        "## 예제 2.2 토큰 아이디에서 벡터로 변환"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dCKTtOt9NvFA",
        "outputId": "66e59720-871a-475f-e966-9a1fc31e37a9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 7, 16])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 16\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "\n",
        "input_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n",
        "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
        "input_embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnYVUo0MXTdc",
        "outputId": "5d84ba8f-4a48-4436-84d1-3f092c019bae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5\n"
          ]
        }
      ],
      "source": [
        "print(len(str2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwClYMSSjLjp"
      },
      "source": [
        "## 예제 2.3 절대적 위치 인코딩"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws1A-ALkjLWH"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 16\n",
        "max_position = 12\n",
        "# 토큰 임베딩 층 생성\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "# 위치 인코딩 층 생성\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
        "\n",
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "position_encodings = position_embed_layer(position_ids)\n",
        "token_embeddings = embed_layer(torch.tensor(input_ids)) # (5, 16)\n",
        "token_embeddings = token_embeddings.unsqueeze(0) # (1, 5, 16)\n",
        "# 토큰 임베딩과 위치 인코딩을 더해 최종 입력 임베딩 생성\n",
        "input_embeddings = token_embeddings + position_encodings\n",
        "input_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MBYHKRMkCDs"
      },
      "source": [
        "## 예제 2.4 쿼리, 키, 값 벡터를 만드는 nn.Linear 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rse5Xy6_jhok"
      },
      "outputs": [],
      "source": [
        "head_dim = 16\n",
        "\n",
        "# 쿼리, 키, 값을 계산하기 위한 변환\n",
        "weight_q = nn.Linear(embedding_dim, head_dim)\n",
        "weight_k = nn.Linear(embedding_dim, head_dim)\n",
        "weight_v = nn.Linear(embedding_dim, head_dim)\n",
        "# 변환 수행\n",
        "querys = weight_q(input_embeddings) # (1, 5, 16)\n",
        "keys = weight_k(input_embeddings) # (1, 5, 16)\n",
        "values = weight_v(input_embeddings) # (1, 5, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wfitct-lkSP2"
      },
      "source": [
        "## 예제 2.5. 스케일 점곱 방식의 어텐션"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ZmhKwmWXTdc",
        "outputId": "f4a6e6b0-8c96-4760-cb05-15d5c6cade30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.3786, -0.5800,  1.0169,  0.9738,  0.2214,  0.7925,  0.5012,\n",
            "           0.1506, -0.8194, -1.3387,  0.7269, -0.1177, -0.8939,  0.8756,\n",
            "          -0.6137, -0.9647],\n",
            "         [ 1.0808, -0.1047, -0.0982,  0.1023,  0.1297,  0.6953,  0.7626,\n",
            "          -1.0505, -0.5499, -1.2547,  0.0382, -0.4115,  0.0724,  0.1315,\n",
            "          -1.4001, -0.3283],\n",
            "         [-0.5575, -0.6064,  0.3807,  0.2899,  0.6409,  0.2535,  0.1195,\n",
            "          -0.7423,  0.7272,  0.0894, -0.8272,  0.2651,  1.4306,  0.1532,\n",
            "           0.6631,  0.7149],\n",
            "         [-0.3763, -0.0448,  0.1174,  0.0965, -0.5448,  0.1974, -0.0835,\n",
            "          -0.4694, -0.6586,  0.4734, -0.2839, -0.1150,  0.1532, -0.9643,\n",
            "           0.0031, -0.5935],\n",
            "         [-0.2383, -0.2460, -0.7402, -0.0127,  0.3045, -0.1004,  0.9954,\n",
            "           0.1625,  0.2350,  0.7611, -0.6246, -0.0646, -0.1221,  0.0544,\n",
            "          -0.2276,  0.8921],\n",
            "         [-0.3403, -0.8502,  0.8460,  0.3944,  0.3689,  0.2789,  0.0879,\n",
            "          -0.5865, -0.6478, -0.9286,  0.3739,  0.0854, -0.8929, -0.2807,\n",
            "          -0.3977, -0.9064],\n",
            "         [-0.2555, -0.5602,  0.1950,  0.3041,  0.3065,  0.9964,  0.8095,\n",
            "           0.6108, -0.9492,  0.1617, -0.1233, -0.2373, -0.6831,  0.8676,\n",
            "          -0.3419, -1.2766]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(querys)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nftEA3lFkSwl"
      },
      "outputs": [],
      "source": [
        "from math import sqrt\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "\tdim_k = querys.size(-1) # 16\n",
        "\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
        "\tweights = F.softmax(scores, dim=-1)\n",
        "\treturn weights @ values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8fk4zA_XTdc",
        "outputId": "510ce97b-140e-44c9-f6df-84782e7b73d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.4591,  0.9048,  0.4076, -0.3501, -0.2559,  0.4911, -0.1154,\n",
            "          -0.9263,  0.2169,  0.0663, -1.0405,  0.9713, -0.7006,  0.3283,\n",
            "           1.1647,  1.7170],\n",
            "         [ 0.0564,  0.0614, -0.4325, -0.6356, -0.3683,  0.4350,  0.1000,\n",
            "          -0.2797, -0.3382, -0.4526,  0.4166, -0.0135, -1.0697,  0.1202,\n",
            "           0.0879,  0.6486],\n",
            "         [-0.4273,  0.3486,  0.3937,  0.2343,  0.1519, -0.8295,  0.2493,\n",
            "           0.2235, -0.1613,  0.7933,  0.7065, -0.2206, -0.4277,  0.4562,\n",
            "          -0.0240, -0.2502],\n",
            "         [-0.2478, -0.1194,  0.3058, -0.5148,  0.2385, -0.4475,  0.0826,\n",
            "          -0.3301, -0.4548,  0.3995,  0.7923, -0.3362, -0.8612,  0.9770,\n",
            "          -0.7764, -0.9706],\n",
            "         [-1.1230,  0.1400,  0.0157, -0.2467,  0.0467, -0.0793,  0.0440,\n",
            "           0.8722, -0.7163,  0.5302,  0.5456, -0.0468,  0.0675, -0.5473,\n",
            "          -0.2705, -0.5187],\n",
            "         [ 0.1290,  0.7769,  0.1402, -1.3459,  0.2560, -0.5554, -0.8203,\n",
            "          -0.0905,  0.1075,  0.0601,  0.0942,  0.3570, -0.4664,  0.7954,\n",
            "           0.2808,  0.4706],\n",
            "         [ 0.5006, -0.4633,  1.2544, -0.5688,  0.5803,  0.1624,  0.3202,\n",
            "          -0.6031, -0.1079,  0.5605, -0.9982, -0.3709, -0.9395,  0.9530,\n",
            "           0.1744,  0.3503]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CzHY8tvlkiTl"
      },
      "source": [
        "## 예제 2.6. 어텐션 연산의 입력과 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h4evxbjRkfIi",
        "outputId": "44629901-1451-4491-a86b-25abc8a3e858"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "원본 입력 형태:  torch.Size([1, 7, 16])\n",
            "어텐션 적용 후 형태:  torch.Size([1, 7, 16])\n"
          ]
        }
      ],
      "source": [
        "print(\"원본 입력 형태: \", input_embeddings.shape)\n",
        "\n",
        "after_attention_embeddings = compute_attention(querys, keys, values)\n",
        "\n",
        "print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)\n",
        "# 원본 입력 형태:  torch.Size([1, 5, 16])\n",
        "# 어텐션 적용 후 형태:  torch.Size([1, 5, 16])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKv4X9rsknXs"
      },
      "source": [
        "## 예제 2.7. 어텐션 연산을 수행하는 AttentionHead 클래스"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3HWTZ4jukn5p"
      },
      "outputs": [],
      "source": [
        "class AttentionHead(nn.Module):\n",
        "  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, head_dim) # 쿼리 벡터 생성을 위한 선형 층\n",
        "    self.weight_k = nn.Linear(token_embed_dim, head_dim) # 키 벡터 생성을 위한 선형 층\n",
        "    self.weight_v = nn.Linear(token_embed_dim, head_dim) # 값 벡터 생성을 위한 선형 층\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    outputs = compute_attention(\n",
        "        self.weight_q(querys),  # 쿼리 벡터\n",
        "        self.weight_k(keys),    # 키 벡터\n",
        "        self.weight_v(values),  # 값 벡터\n",
        "        is_causal=self.is_causal\n",
        "    )\n",
        "    return outputs\n",
        "\n",
        "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
        "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vw_iPsbtXTdd",
        "outputId": "bf13dc94-9c0c-4046-f17b-bff61163bf85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AttentionHead(\n",
            "  (weight_q): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (weight_k): Linear(in_features=16, out_features=16, bias=True)\n",
            "  (weight_v): Linear(in_features=16, out_features=16, bias=True)\n",
            ")\n",
            "tensor([[[-0.0690,  0.2420, -0.2890,  0.0628,  0.1173, -0.0031,  0.1127,\n",
            "          -0.0609, -0.0318, -0.1841,  0.2240, -0.1713,  0.2732,  0.2338,\n",
            "          -0.2762, -0.0900],\n",
            "         [-0.0884,  0.2936, -0.2547, -0.0983, -0.0491,  0.0967,  0.1677,\n",
            "          -0.2342, -0.0785, -0.2241,  0.2499, -0.0358,  0.5087,  0.2882,\n",
            "          -0.1870, -0.0426],\n",
            "         [-0.0621,  0.2846, -0.2221, -0.1026, -0.0680,  0.0997,  0.1711,\n",
            "          -0.2265, -0.0712, -0.2080,  0.2572, -0.0507,  0.5073,  0.3028,\n",
            "          -0.1828, -0.0147],\n",
            "         [-0.0755,  0.3013, -0.2008, -0.1500, -0.1463,  0.1539,  0.1797,\n",
            "          -0.2812, -0.0926, -0.2159,  0.2490, -0.0067,  0.6047,  0.3327,\n",
            "          -0.1395,  0.0311],\n",
            "         [-0.0566,  0.3002, -0.2104, -0.1297, -0.1246,  0.1374,  0.1675,\n",
            "          -0.2854, -0.0941, -0.2118,  0.2651, -0.0332,  0.5696,  0.3236,\n",
            "          -0.1618, -0.0156],\n",
            "         [-0.0389,  0.2518, -0.2264,  0.0026,  0.0020,  0.0606,  0.1174,\n",
            "          -0.1176, -0.0530, -0.1690,  0.2373, -0.1579,  0.3754,  0.2807,\n",
            "          -0.2288, -0.0154],\n",
            "         [-0.0708,  0.2593, -0.2242, -0.0387, -0.0385,  0.1065,  0.1605,\n",
            "          -0.1416, -0.1104, -0.1691,  0.2343, -0.0528,  0.4698,  0.2674,\n",
            "          -0.2255,  0.0168]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(attention_head)\n",
        "print(after_attention_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30IXVnNElE2O"
      },
      "source": [
        "## 예제 2.8. 멀티 헤드 어텐션 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-qTbFVMlFND"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "  def __init__(self, token_embed_dim, d_model, n_head, is_causal=False):\n",
        "    super().__init__()\n",
        "    self.n_head = n_head\n",
        "    self.is_causal = is_causal\n",
        "    self.weight_q = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_k = nn.Linear(token_embed_dim, d_model)\n",
        "    self.weight_v = nn.Linear(token_embed_dim, d_model)\n",
        "    self.concat_linear = nn.Linear(d_model, d_model)\n",
        "\n",
        "  def forward(self, querys, keys, values):\n",
        "    B, T, C = querys.size()\n",
        "    querys = self.weight_q(querys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    keys = self.weight_k(keys).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    values = self.weight_v(values).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "    attention = compute_attention(querys, keys, values, self.is_causal)\n",
        "    output = attention.transpose(1, 2).contiguous().view(B, T, C)\n",
        "    output = self.concat_linear(output)\n",
        "    return output\n",
        "\n",
        "n_head = 4\n",
        "mh_attention = MultiheadAttention(embedding_dim, embedding_dim, n_head)\n",
        "after_attention_embeddings = mh_attention(input_embeddings, input_embeddings, input_embeddings)\n",
        "after_attention_embeddings.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWtHyqa_mAtB"
      },
      "source": [
        "## 예제 2.9. 층 정규화 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ikXwtWFBl5zw",
        "outputId": "93392ff7-2e59-4ac0-b817-615858508a3e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor([[-2.9802e-08, -1.4901e-08,  0.0000e+00, -1.8626e-08, -2.2352e-08,\n",
              "           0.0000e+00,  7.4506e-09]]),\n",
              " tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "norm = nn.LayerNorm(embedding_dim)\n",
        "norm_x = norm(input_embeddings)\n",
        "norm_x.shape # torch.Size([1, 5, 16])\n",
        "\n",
        "norm_x.mean(dim=-1).data, norm_x.std(dim=-1).data\n",
        "\n",
        "# (tensor([[ 2.2352e-08, -1.1176e-08, -7.4506e-09, -3.9116e-08, -1.8626e-08]]),\n",
        "#  tensor([[1.0328, 1.0328, 1.0328, 1.0328, 1.0328]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkeIvwCYnSOs"
      },
      "source": [
        "## 예제 2.10. 피드 포워드 층 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3e9702XvnSrT"
      },
      "outputs": [],
      "source": [
        "class PreLayerNormFeedForward(nn.Module):\n",
        "  def __init__(self, d_model, dim_feedforward, dropout):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(d_model, dim_feedforward) # 선형 층 1\n",
        "    self.linear2 = nn.Linear(dim_feedforward, d_model) # 선형 층 2\n",
        "    self.dropout1 = nn.Dropout(dropout) # 드랍아웃 층 1\n",
        "    self.dropout2 = nn.Dropout(dropout) # 드랍아웃 층 2\n",
        "    self.activation = nn.GELU() # 활성 함수\n",
        "    self.norm = nn.LayerNorm(d_model) # 층 정규화\n",
        "\n",
        "  def forward(self, src):\n",
        "    x = self.norm(src)\n",
        "    x = x + self.linear2(self.dropout1(self.activation(self.linear1(x))))\n",
        "    x = self.dropout2(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq3eJqRInWWS"
      },
      "source": [
        "## 예제 2.11. 인코더 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNCFpdVknUVa"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, nhead, dim_feedforward, dropout):\n",
        "    super().__init__()\n",
        "    self.attn = MultiheadAttention(d_model, d_model, nhead) # 멀티 헤드 어텐션 클래스\n",
        "    self.norm1 = nn.LayerNorm(d_model) # 층 정규화\n",
        "    self.dropout1 = nn.Dropout(dropout) # 드랍아웃\n",
        "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout) # 피드포워드\n",
        "\n",
        "  def forward(self, src):\n",
        "    norm_x = self.norm1(src)\n",
        "    attn_output = self.attn(norm_x, norm_x, norm_x)\n",
        "    x = src + self.dropout1(attn_output) # 잔차 연결\n",
        "\n",
        "    # 피드 포워드\n",
        "    x = self.feed_forward(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7acyE0lnc5L"
      },
      "source": [
        "## 예제 2.12. 인코더 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty7TTF55nYDr"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, encoder_layer, num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = get_clones(encoder_layer, num_layers)\n",
        "    self.num_layers = num_layers\n",
        "    self.norm = norm\n",
        "\n",
        "  def forward(self, src):\n",
        "    output = src\n",
        "    for mod in self.layers:\n",
        "        output = mod(output)\n",
        "    return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dJpZJGrnhMI"
      },
      "source": [
        "## 예제 2.13. 디코더에서 어텐션 연산(마스크 어텐션)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2nBX5monelI"
      },
      "outputs": [],
      "source": [
        "def compute_attention(querys, keys, values, is_causal=False):\n",
        "\tdim_k = querys.size(-1) # 16\n",
        "\tscores = querys @ keys.transpose(-2, -1) / sqrt(dim_k) # (1, 5, 5)\n",
        "\tif is_causal:\n",
        "\t\tquery_length = querys.size(2)\n",
        "\t\tkey_length = keys.size(2)\n",
        "\t\ttemp_mask = torch.ones(query_length, key_length, dtype=torch.bool).tril(diagonal=0)\n",
        "\t\tscores = scores.masked_fill(temp_mask == False, float(\"-inf\"))\n",
        "\tweights = F.softmax(scores, dim=-1) # (1, 5, 5)\n",
        "\treturn weights @ values # (1, 5, 16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jxCS_lunl_7"
      },
      "source": [
        "## 예제 2.14. 크로스 어텐션이 포함된 디코더 층"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b7youbG9njnW"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "  def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.self_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "    self.multihead_attn = MultiheadAttention(d_model, d_model, nhead)\n",
        "    self.feed_forward = PreLayerNormFeedForward(d_model, dim_feedforward, dropout)\n",
        "\n",
        "    self.norm1 = nn.LayerNorm(d_model)\n",
        "    self.norm2 = nn.LayerNorm(d_model)\n",
        "    self.dropout1 = nn.Dropout(dropout)\n",
        "    self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "  def forward(self, tgt, encoder_output, is_causal=True):\n",
        "    # 셀프 어텐션 연산\n",
        "    x = self.norm1(tgt)\n",
        "    x = x + self.dropout1(self.self_attn(x, x, x, is_causal=is_causal))\n",
        "    # 크로스 어텐션 연산\n",
        "    x = self.norm2(x)\n",
        "    x = x + self.dropout2(self.multihead_attn(x, encoder_output, encoder_output))\n",
        "    # 피드 포워드 연산\n",
        "    x = self.feed_forward(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l218C0ZOnqDO"
      },
      "source": [
        "## 예제 2.15. 디코더 구현"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7meGa10vnnw1"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "def get_clones(module, N):\n",
        "  return nn.ModuleList([copy.deepcopy(module) for i in range(N)])\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "  def __init__(self, decoder_layer, num_layers):\n",
        "    super().__init__()\n",
        "    self.layers = get_clones(decoder_layer, num_layers)\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "  def forward(self, tgt, src):\n",
        "    output = tgt\n",
        "    for mod in self.layers:\n",
        "        output = mod(output, src)\n",
        "    return output"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}